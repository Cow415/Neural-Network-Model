{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e16dba",
   "metadata": {},
   "source": [
    "# Final Project-Building NN From Scratch\n",
    "Work from this script, pull basic function and classes from other built script. \n",
    "\\\n",
    "First: Let's import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d92b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append('os.getcwd()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05e46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in built modules\n",
    "from layers.conv import Conv2D\n",
    "from layers.relu import ReLU\n",
    "from layers.pool import MaxPool2D\n",
    "from layers.flatten import Flatten\n",
    "from layers.dense import Dense\n",
    "\n",
    "from model.sequential import SequentialModel\n",
    "from loss.softmax_ce import SoftmaxCrossEntropyLoss\n",
    "from optim.adam import Adam\n",
    "\n",
    "from data.dataset import MNISTDataset\n",
    "from data.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f3fef",
   "metadata": {},
   "source": [
    "Assume the the file is unloaded as npz. \n",
    "\n",
    "Load in the MNIST Data; then pull in the Dataset and DataLoader functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d9d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 1, 28, 28) (60000,)\n",
      "Testing data shape: (10000, 1, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data\n",
    "data = np.load(\"/Users/caozehuang/Documents/GitHub/Neural-Network-Model/data/mnist.npz\")\n",
    "X_train = data['x_train']  # Note: lowercase keys from unpacker\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Check Shapes\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e821dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crease Dataset and DataLoader\n",
    "train_dataset = MNISTDataset(X_train, y_train)\n",
    "test_dataset = MNISTDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02de19",
   "metadata": {},
   "source": [
    "Construct the CNN class and propagations using the sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc72c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel([\n",
    "    Conv2D(1, 8, 3, padding=1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "\n",
    "    Conv2D(8, 16, 3, padding=1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(16 * 7 * 7, 128),\n",
    "    ReLU(),\n",
    "    Dense(128, 10)           \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ce9f",
   "metadata": {},
   "source": [
    "Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39dafaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftmaxCrossEntropyLoss()\n",
    "optimizer = Adam(model.params(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f208d",
   "metadata": {},
   "source": [
    "Training Loop for set epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2197ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX, y = next(iter(train_loader))\\nlogits = model.forward(X)\\nprint(\"Logit shape: \", logits.shape)\\n\\ngradient = criterion.backward()\\nmodel.backward(gradient)\\n\\nprint(\"Backward pass completed.\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugging Segments----Ignore---\n",
    "\"\"\"\n",
    "X, y = next(iter(train_loader))\n",
    "logits = model.forward(X)\n",
    "print(\"Logit shape: \", logits.shape)\n",
    "\n",
    "gradient = criterion.backward()\n",
    "model.backward(gradient)\n",
    "\n",
    "print(\"Backward pass completed.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0366e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.1353, acc=0.9592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mforward(logits, y)\n\u001b[1;32m      8\u001b[0m     grad \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Model/cnn_comp/model/sequential.py:16\u001b[0m, in \u001b[0;36mSequentialModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mDefine the forward pass through all layers\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Model/cnn_comp/layers/conv.py:50\u001b[0m, in \u001b[0;36mConv2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 h \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\n\u001b[1;32m     49\u001b[0m                 w \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\n\u001b[0;32m---> 50\u001b[0m                 out[n,f,i,j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     51\u001b[0m                     x_pad[n,:,h:h\u001b[38;5;241m+\u001b[39mKH,w:w\u001b[38;5;241m+\u001b[39mKW] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[f]\n\u001b[1;32m     52\u001b[0m                 ) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[f]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2344\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `min` or `max` keyword argument when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2339\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2345\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2351\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    correct = total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        logits = model.forward(X)\n",
    "        loss = criterion.forward(logits, y)\n",
    "        grad = criterion.backward()\n",
    "\n",
    "        model.backward(grad)\n",
    "        optimizer.step(model.grads())\n",
    "\n",
    "        running_loss += loss * len(y)\n",
    "        correct += (logits.argmax(axis=1) == y).sum()\n",
    "        total += len(y)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: \"\n",
    "        f\"loss={running_loss/total:.4f}, \"\n",
    "        f\"acc={correct/total:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30597e62",
   "metadata": {},
   "source": [
    "Evaluation and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374963fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Check\n",
    "correct = total = 0\n",
    "\n",
    "for X, y in test_loader:\n",
    "    logits = model.forward(X)\n",
    "    preds = logits.argmax(axis=1)\n",
    "    correct += (preds == y).sum()\n",
    "    total += len(y)\n",
    "\n",
    "print(f\"Test accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfit Debug\n",
    "tiny_ds = MNISTDataset(X_train[:10], y_train[:10])\n",
    "tiny_loader = DataLoader(tiny_ds, batch_size=5, shuffle=True)\n",
    "# If 100% accuracy not reached, backward bug exists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
